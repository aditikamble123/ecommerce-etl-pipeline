ğŸ›’ E-Commerce ETL Pipeline (Python + MySQL)
ğŸ“Œ Overview

This project implements an end-to-end ETL (Extract, Transform, Load) pipeline for e-commerce order data.

Raw CSV data is ingested, cleaned, transformed, and loaded into a MySQL data warehouse designed using a star schema to support analytical queries.

The pipeline is modular, production-oriented, and Airflow-ready for orchestration in a Linux environment.

ğŸ—ï¸ Architecture
CSV File
   â†“
Extract (Python)
   â†“
Transform (Cleaning, Validation, Normalization)
   â†“
Load (MySQL Data Warehouse)
   â†“
Analytics Queries

ğŸ› ï¸ Tech Stack

Python

Pandas

MySQL

SQLAlchemy

Apache Airflow (design-level orchestration)

Git & GitHub

ğŸ“‚ Project Structure
ecommerce-etl-pipeline/
â”‚
â”œâ”€â”€ etl/
â”‚   â”œâ”€â”€ extract.py          # Data extraction logic
â”‚   â”œâ”€â”€ transform.py        # Data cleaning & transformation
â”‚   â””â”€â”€ load.py             # Load data into MySQL
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ ecommerce_orders_10k_updated.csv
â”‚
â”œâ”€â”€ sql/
â”‚   â””â”€â”€ create_tables.sql
â”‚
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ etl_pipeline.py     # Airflow DAG (design-level)
â”‚
â”œâ”€â”€ test_etl.py             # Run ETL without Airflow
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md

ğŸ§  Business Problem

E-commerce order data arrives as raw CSV files that may contain:

Duplicates

Inconsistent formats

Redundant fields

Business teams need clean, structured, and query-optimized data to analyze:

Revenue trends

Customer segments

Country-wise sales

Product performance

This project automates the conversion of raw data into analytics-ready tables.

ğŸ—„ï¸ Data Warehouse Design (Star Schema)
Dimension Tables

customers_dim

user_id

country

customer_segment

products_dim

product_id

category

price

date_dim

date_id

order_date

year

month

day

Fact Table

orders_fact

order_id

user_id

product_id

date_id

qty

total_price

ğŸ”„ ETL Workflow
1ï¸âƒ£ Extract

Reads raw CSV data using Pandas

2ï¸âƒ£ Transform

Removes duplicates

Handles missing and invalid values

Converts date formats

Creates dimension and fact tables

Applies data quality validations

3ï¸âƒ£ Load

Loads transformed data into MySQL

Uses chunked inserts for reliability (Windows-friendly)

Designed for safe re-runs during development

â–¶ï¸ How to Run the Project
1ï¸âƒ£ Create virtual environment
python -m venv venv

2ï¸âƒ£ Activate virtual environment
# Windows
venv\Scripts\activate

3ï¸âƒ£ Install dependencies
pip install -r requirements.txt

4ï¸âƒ£ Set MySQL password (environment variable)
set MYSQL_PASSWORD=your_mysql_password

5ï¸âƒ£ Run ETL pipeline
python test_etl.py

âœ… Expected Output
Starting ETL process...
Extracted 10000 rows
Transformation completed
Loading customers_dim...
Loading products_dim...
Loading date_dim...
Loading orders_fact...
Load step completed
ETL ran successfully

ğŸ” Verify Data in MySQL
USE ecommerce_dw;

SELECT COUNT(*) FROM orders_fact;
SELECT * FROM customers_dim LIMIT 5;

ğŸ“Š Sample Analytics Query
SELECT
    country,
    SUM(total_price) AS total_revenue
FROM orders_fact o
JOIN customers_dim c
    ON o.user_id = c.user_id
GROUP BY country
ORDER BY total_revenue DESC;

â° Airflow Orchestration

An Apache Airflow DAG is included for scheduling and orchestration.

Due to Windows limitations, the DAG is design-level only, but it is fully compatible with Linux / WSL-based Airflow deployments.

ğŸš€ Key Learnings

Built modular ETL pipelines using Python

Designed a star-schema data warehouse

Handled real-world dependency and driver issues

Implemented chunked bulk loading for MySQL

Designed Airflow-ready orchestration
